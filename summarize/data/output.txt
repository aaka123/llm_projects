Current long context large language models (LLMs) can process extensive inputs (over 100,000 tokens) but struggle to generate outputs exceeding 2,000 words. Through experiments, the authors found this limitation is due to the scarcity of long-output examples in their supervised fine-tuning (SFT) datasets, rather than an inherent architectural limitation.

To address this, they developed AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into smaller subtasks, allowing off-the-shelf LLMs to produce coherent outputs of over 20,000 words. AgentWrite first creates a detailed writing plan and then generates content for each paragraph sequentially, ensuring coherence.

Leveraging AgentWrite, they constructed LongWriter-6k, a dataset of 6,000 SFT examples with output lengths ranging from 2,000 to 32,000 words. Integrating LongWriter-6k into model training successfully scales the output length of existing models to over 10,000 words while maintaining quality. They also introduced LongBench-Write, a comprehensive benchmark to evaluate ultra-long generation capabilities.

Their 9B parameter model, further improved with Direct Preference Optimization (DPO), achieved state-of-the-art performance on LongBench-Write, surpassing larger proprietary models. Ablation studies confirmed that the LongWriter-6k dataset is crucial for enabling long outputs, while approaches like explicitly outputting writing plans or using instruction backtranslation from low-quality texts were less effective or detrimental.

The work concludes that existing long-context LLMs possess the potential for larger output windows, which can be unlocked by providing sufficient long-output data during model alignment.